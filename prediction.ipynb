{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Define Paths\n",
                "This isn't necessary. It was something I started doing so to easily switch between different versions of the model\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "PROJECT_ROOT_DIR = '/home/rharshal/myProjects/thesis/deepspeech2-prediction'\n",
                "\n",
                "CUSTOM_FILES_DIR = f'{PROJECT_ROOT_DIR}/src'\n",
                "\n",
                "DATA_DIR = f'{PROJECT_ROOT_DIR}/data/'\n",
                "MODEL_DIR = f'{PROJECT_ROOT_DIR}/ml-models/walltrainedon1305'\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "model_weights_path = f'{MODEL_DIR}/wallmodel1305.h5'\n",
                "feature_mean_path = f'{MODEL_DIR}/mean-wall1305.pickle'\n",
                "feature_stddev_path = f'{MODEL_DIR}/std-wall1305.pickle'"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Perform Imports"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "import os, sys\n",
                "sys.path.append(CUSTOM_FILES_DIR)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "from create_model import final_model\n",
                "from data_generator import AudioGenerator\n",
                "from utils import int_sequence_to_text\n",
                "\n",
                "from keras import backend as K\n",
                "from keras.layers import GRU\n",
                "\n",
                "import pickle"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load Preprocessing objects"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "with open(feature_mean_path, 'rb') as fp:\n",
                "    feat_mean = pickle.load(fp)\n",
                "\n",
                "with open(feature_stddev_path, 'rb') as fp:\n",
                "    feat_std = pickle.load(fp)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load Model"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "model = final_model(input_dim=161,\n",
                "                        filters=200,\n",
                "                        kernel_size=11, \n",
                "                        conv_stride=2,\n",
                "                        conv_border_mode='same',\n",
                "                        units=250,\n",
                "                        # activation='relu',\n",
                "                        cell=GRU,\n",
                "                        dropout_rate=0.2,\n",
                "                        number_of_layers=2,\n",
                "                        conv_layers=2)\n",
                "                        "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "model.load_weights(model_weights_path)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Do data files loading related stuff"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "def normalize_test(feature,mean,std,eps=1e-14):\n",
                "    return (feature - mean) / (std + eps)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Save file-names for easier re-trials"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "audio_filenames = [\n",
                "    'boliye_1574414678229.wav',  'kitne_1574413891009.wav', 'nahimila_1574415020378.wav', 'order_1574414227763.wav',\n",
                "    'kamhai_1574414938995.wav', 'milgaya_1574415168217.wav', 'number_1574414716726.wav', 'test.wav',\n",
                "    'my_kitne.wav', 'my_boliye.wav', 'my_kamhai.wav', 'my_milgaya_1.wav', 'my_milgaya_2.wav']\n",
                "\n",
                "audio_path = f'{DATA_DIR}/some-files/{audio_filenames[-4]}'\n",
                "print(audio_path)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "/home/rharshal/myProjects/thesis/deepspeech2-prediction/data//some-files/my_boliye.wav\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Create data generator class"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "data_gen = AudioGenerator(spectrogram=True)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load a file from the above list"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "data_point = data_gen.featurize(audio_path)\n",
                "data_point_normalized = normalize_test(data_point,feat_mean, feat_std)\n",
                "data_point_normalized.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(175, 161)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 11
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Do Prediction"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "# Pass the spectrogram through the model\n",
                "pred = model.predict(np.expand_dims(data_point_normalized, axis=0))\n",
                "print(f'model output shape: {pred.shape}')\n",
                "\n",
                "# Get additional info about the input, used for CTC-decoding below\n",
                "output_length = [model.output_length(data_point.shape[0])] \n",
                "print(f'{output_length}')\n",
                "\n",
                "# Do CTC decoding\n",
                "pred_ints = (K.eval(K.ctc_decode(pred, output_length)[0][0])+1).flatten().tolist()\n",
                "pred_l = int_sequence_to_text(pred_ints)\n",
                "pred_text=''.join(pred_l)\n",
                "\n",
                "print(f'Model output decoded: {pred_text}')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "model output shape: (1, 44, 29)\n",
                        "[44]\n",
                        "WARNING:tensorflow:From /home/rharshal/miniconda3/envs/cnn_rnn/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:5871: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
                        "Instructions for updating:\n",
                        "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
                        "Model output decoded: boliye\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.6.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6.8 64-bit ('cnn_rnn': conda)"
        },
        "interpreter": {
            "hash": "8c9873bcc9ebb1b53c79c9b651d8a27c737712e5277a606925e1f19078210637"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}